## The recommended configs of host computers and management console running Debian GNU/Linux within clusters.
##### Copyright (c) TYA
##### Homepage: http://tya.company/

##### Partitioning and file system for hosts.

I personally suggest to use a 24~32GiB SSD to store the code and static data (both could be provided by the package management system), and a much larger HDD to store data generated by daemons and users.

It is needless to say that the most capacity-consuming storage on a virtual machine host is the storage pool for disk images, but in a cluster it usually is a specific infrastructure shared between every single host (see below for further discussion), so there will not be so much data needed to store inside one host's OWN (not shared) storage. Because of that, using a single large SSD to store all data needed for a host is also feasible.

From now on, I assume the small SSD is `sda`, and the large HDD is `sdb`. It only needs small adjust to adapt to single-disk scheme.

Using GPT on both disk is recommended, and a BIOS boot partition had better be put just behind the first GPT, usually between the [34, 2047] sectors.

On `sda`, behind the BIOS boot partition lie `/boot/efi`, `/boot`, `/var/lib/dpkg`, `/` and `/usr/local` in order.

I usually leave 34MiB for `/boot/efi`, `/boot` could be 512MiB to store kernels and initramfses, `/var/lib/dpkg` is where to store essential metadata for debian package management system (The dpkg), which needs around 1GiB and should be kept in the same disk with root partition so that they are easy to migrate along side in order to keep integrity, and the remaining capacity is to be shared between the root partition and `/usr/local`. Those two partition lie along side, so it is easy to adjust the boundry between them.

On `sdb`, behind the BIOS boot partition lie the swap partition, `/var/lib/dpkg/updates`, `/var` and `/home`.

`/var/lib/dpkg/updates` is used to store temporary files and contexts for dpkg(1) to perform package management. Its contents must survive till the next time dpkg(1) being called, in case of system crash during package management, for dpkg(1) to recover, so it MUST not be a tmpfs, and had better be separated from `/var/lib/dpkg` if double-disk scheme is used to avoid unnecessary wearing of SSD. (On single-disk systems there is no need to separate `/var/lib/dpkg/updates` from its parent directory, and their capacity should be combined.) I usually leave 1GiB for it.

The remaining capacity is to be shared between `/var` and `/home`.

Every partition on SSD had better be mounted with the option `discard` if the SSD and the file system they use support that BOTH, and every partition should mounted with the option `user_xattr` if supported, in order to make it easy to deploy grsecurity later.

##### Repositories setup.

TBF

##### Configs applied after installation but before reboot.

TBF

##### Install necessary packages needed for virtual mechine hosts.

TBF

##### Use a shared storage pool to store disk images of virtual machines.

`libvirt`'s "migrate" action can only migrate the definition, as well as the whole state when performing a live migration. In reality, libvirt assumes that **the STORAGE POOL is shared between the source and the destination hosts, and mounted to the same path, the images should remain accessible via the very same path when the migration is done**, otherwise the migration will fail to start. So, the easiest way to config the hosts is mounting a shared storage (NFS and the like) on the path `/var/lib/libvirt/images`, where the pool `default` is defined, of each hosts, making them effectively a cluster.

For example, if the shared storage is an NFS, mount it to `/var/lib/libvirt/images` with the following instruction:

`$hostname_of_nfs_server.local:/the/exported/path/for/nfs	/var/lib/libvirt/images	nfs	auto	0	0`

Source images (e.g. isos for installation) can be put into the pool using `vol-upload` sub-command of `virsh(1)`, and you can get a backup of one image inside the pool using `vol-download` sub-command.

##### Use a normal user to perform libvirt-related maintenance.

There are a lot of documents related to libvirt in which `root` user is used to perform guest-related maintenance, but in reality, those are bad practices.

In Debian GNU/Linux, permissions needed to manage virtual machines are assigned to group `libvirt` and `kvm`, so you should create a user to be specialized to manage vms, e.g. `virtmgr`, and add it to those two group above:

```
# adduser virtmgr
# usermod -aG libvirt virtmgr
# usermod -aG kvm virtmgr
```

libvirt defaults to qemu:///session for non-root. So from `virtmgr` you'll need to do: 

`$ virsh --connect qemu:///system list --all`

You can use environment variable `LIBVIRT_DEFAULT_URI` to change this. 

Such user account is feasible to perform remote management via ssh. You can use `virsh`, `virt-manager` or other tools based on libvirt to conect to the host via this account to perform everything libvirt provides, e.g.

`$ virsh --connect qemu+ssh://virtmgr@$HOSTNAME_OF_THE_HOST.local/system ...`

##### Use mdns to make it possible to access computers via domain names derived from their hostname instead of ip address.

By deploying [mdns](https://en.wikipedia.org/wiki/Multicast_DNS) (one of whose famous implementation is the Bonjour of Apple Inc) server on each computers within the same subnet, they can contact each other by using domain names in a format like "**$HOSTNAME_OF_THE_TARGET_HOST.local**".

I believe it is needless to say that domain name derived from hostname is easier to remember than ip address.

The major implementation of mdns on most Unix-like operation systems is [Avahi](https://en.wikipedia.org/wiki/Avahi_%28software%29). In Debian GNU/Linux, it is divided to a lot of packages. To make use of the most basic function of mdns, you could just install `avahi-daemon` as mdns server and `libnss-mdns` to interface mdns name resolution to [Name Service Switch](https://en.wikipedia.org/wiki/Name_Service_Switch):

`# apt-get install avahi-daemon libnss-mdns`

Enjoy mdns name resolution after deploying them on every host within your subnet!

##### Making the management console of the cluster able to mount the shared storage pool may be benefitting.

A management console of a cluster is a computer for the cluster administrators to log in, able to access and manage hosts within the cluster, through which the guests living upon the hosts get managed. But how could it become if the console also mounts the shared storage pool used by worker hosts, and becomes a functional host itself?

Virtual machines can then be created and calibrated on the management console, and can be *migrated* onto an appropriate worker host once it is feasible for production use. Malfunctional but running guests can also be *migrated* onto the management console to check and repair.

Disks images can be uploaded and downloaded "locally" if the management console directly mounts the shared storage pool, eliminating the communicational expense between the console and a worker host.

######Reference: 
######[1] https://wiki.debian.org/libvirt
######[2] https://wiki.debian.org/KVM
######[3] https://libvirt.org/virshcmdref.html
######[4] https://docs.fedoraproject.org/en-US/Fedora/13/html/Virtualization_Guide/chap-Virtualization-KVM_live_migration.html#sect-Virtualization-KVM_live_migration-Live_migration_requirements
######[5] https://docs.fedoraproject.org/en-US/Fedora_Draft_Documentation/0.1/html/Virtualization_Deployment_and_Administration_Guide/App_Migration_Disk_Image.html
